{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG EVALUATION\n",
    "\n",
    "Nous allons évaluer notre système de RAG de tourisme au Cameroun.\n",
    "\n",
    "1. Nous allons dans un premier temps construire le système de RAG.\n",
    "2. Construire un jeu de données synthétique pour l'évaluation.\n",
    "3. Evaluer avec un llm-as-judge\n",
    "4. Centraliser nos évaluations sur mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Librairies utiles et variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.documentPreparation import prepare_rag_data\n",
    "from src.agenthandler import (\n",
    "    instanciate_llm_with_huggingface, \n",
    "    initialize_embeddings_model,\n",
    "    answer_with_rag,\n",
    "    run_rag_tests,\n",
    "    evaluate_answers\n",
    "    )\n",
    "import pandas as pd\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of documents in file folder\n",
    "import os\n",
    "\n",
    "FILES = [f\"files/{f}\" for f in os.listdir('files') ]\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'  #'sentence-transformers/all-mpnet-base-v2' # 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 50\n",
    "MAX_NEW_TOKENS = 500\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.9\n",
    "REPETITION_PENALTY = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Jeu de données d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"what are cameroonian traditionnal meals ?\",\n",
    "    \"How many museum west region of Cameroon has ?\",\n",
    "    \"what are the main waterfall in west region of Cameroon ?\",\n",
    "    \"Does cameroon has cliff ?\",\n",
    "    \"Can you give me some ecotourism site in Cameroon ?\",\n",
    "    \"Where is located Cameroon national museum ?\",\n",
    "    \n",
    "    \"What is Cameroon population ?\",\n",
    "    \"What is Cameroon land area ?\",\n",
    "    \"when is Cameroon national celebration ?\",\n",
    "    \n",
    "]\n",
    "\n",
    "answers = [\n",
    "    \"Cameroon traditional meals : ndole, eru, okok, kwem, Achu Soup (Yellow Soup) & Achu, poulet DG.\",\n",
    "    \"12\",\n",
    "    \"Ekom nkam waterfall\",\n",
    "    \"yes\",\n",
    "    \"Korup National Park, Ebogo tourist site, Dja Faunal Reserve National Park\",\n",
    "    \"Yaoundé\",\n",
    "    \"26.545.863 \",\n",
    "    \"183,569 square miles\",\n",
    "    \"20th of May\"\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\" : questions,\n",
    "        \"answer\": answers\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation du système de RAG\n",
    "\n",
    "La première étape de la construction d'un système de RAG consiste en la préparation des données. La préparation inclut : \n",
    "\n",
    "1. La lecture des documents, elle se fait en utilisant la classe PyPDFLoader de langchain\n",
    "\n",
    "2. Le découpage du texte en petit bout appelés chunks. Nous allons utiliser le RecursiveCharacterTextSplitter de langchain.\n",
    "\n",
    "3. Le calcul des embeddings avec un modèle pré-entrainé disponible sur huggingface\n",
    "\n",
    "4. Le stockage de ces embeddings dans une base de données vectorielle.\n",
    "\n",
    "Le `RecursiveCharacterTextSplitter` dans LangChain divise les documents en appliquant de manière récursive une série de séparateurs pour découper le texte en morceaux plus petits. Voici une explication étape par étape de son fonctionnement :\n",
    "\n",
    "1. **Définir les séparateurs** :\n",
    "   Vous fournissez une liste de séparateurs (par exemple, `[\"\\n\\n\", \"\\n\", \" \", \"\"]`). Le séparateur essaiera de diviser le texte au premier séparateur de la liste. S'il ne peut pas diviser le texte en morceaux de la taille souhaitée en utilisant ce séparateur, il passera au suivant, et ainsi de suite.\n",
    "\n",
    "2. **Division récursive** :\n",
    "   Le séparateur commence par le plus grand séparateur (par exemple, `\"\\n\\n\"` pour les paragraphes) et tente de diviser le texte en morceaux plus petits que la taille spécifiée (`chunk_size`). Si les morceaux résultants sont encore trop grands, il utilisera le séparateur suivant dans la liste (par exemple, `\"\\n\"` pour les lignes) pour diviser davantage le texte.\n",
    "\n",
    "3. **Taille des morceaux et chevauchement** :\n",
    "   Vous spécifiez une `chunk_size` (taille maximale de chaque morceau) et un `chunk_overlap` (nombre de caractères qui doivent chevaucher entre les morceaux consécutifs). Le séparateur s'assure que chaque morceau est dans la taille spécifiée et inclut le chevauchement pour maintenir le contexte entre les morceaux.\n",
    "\n",
    "4. **Morceaux finaux** :\n",
    "   Le processus continue jusqu'à ce que le texte soit divisé en morceaux qui sont tous dans la taille souhaitée. Les morceaux finaux sont ensuite retournés sous forme de liste.\n",
    "\n",
    "Cette manière de faire le chunks assure de garder le maximum de cohérence après le découpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = initialize_embeddings_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instanciate_llm_with_huggingface(\n",
    "        model_name = \"mistralai/Mistral-7B-Instruct-v0.3\",  \n",
    "        max_new_tokens = MAX_NEW_TOKENS,\n",
    "        do_sample = DO_SAMPLE,\n",
    "        temperature  = TEMPERATURE,\n",
    "        top_p = TOP_P,\n",
    "        repetition_penalty = REPETITION_PENALTY\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_name = \"databricks/dolly-v2-12b\" #\"meta-llama/Llama-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbial\\AppData\\Local\\Temp\\ipykernel_13324\\3447722831.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"GPT4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-100_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Index not found, generating it...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:17,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:34<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-100_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:30<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-100_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:17,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:30<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-100_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:31<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-100_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:16,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:40<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-100_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:30<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-200_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:33<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-200_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:35<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-200_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:31<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-200_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01,  8.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:33<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-200_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:38<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-200_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:34<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-300_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:30<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-300_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:28<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-300_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:28<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-300_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:36<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-300_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:37<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk-300_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2:\n",
      "Loading knowledge base embeddings...\n",
      "Reading pdf files...\n",
      "Chunking the documents...\n",
      "Initializing embeddings model...\n",
      "Creating vectorial db...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:31<00:00,  3.49s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [100,200, 300]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for temperature in [0.4, 0.8, 1.0]:\n",
    "        for embeddings in [MODEL_NAME]:  # Add other embeddings as needed\n",
    "            for rerank in [True, False]:\n",
    "                settings_name = f\"chunk-{chunk_size}_temperature-{temperature}_embeddings-{embeddings.replace('/', '~')}_rerank-{rerank}_reader-model-{MODEL_NAME}\"\n",
    "                output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "                print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "                print(\"Loading knowledge base embeddings...\")\n",
    "\n",
    "                knowledge_index = prepare_rag_data(\n",
    "                    list_file_path = FILES, \n",
    "                    chunk_size = chunk_size,\n",
    "                    chunk_overlap = CHUNK_OVERLAP,\n",
    "                    model_name = embeddings,\n",
    "                    embedding_model=embedding_model,\n",
    "                )\n",
    "\n",
    "                print(\"Running RAG...\")\n",
    "                model = instanciate_llm_with_huggingface(\n",
    "                    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\",  \n",
    "                    max_new_tokens = MAX_NEW_TOKENS,\n",
    "                    do_sample = DO_SAMPLE,\n",
    "                    temperature  = temperature,\n",
    "                    top_p = TOP_P,\n",
    "                    repetition_penalty = REPETITION_PENALTY\n",
    "                    )\n",
    "                run_rag_tests(\n",
    "                    eval_dataset=eval_df,\n",
    "                    llm=model,\n",
    "                    knowledge_index=knowledge_index,\n",
    "                    output_file=output_file_name,\n",
    "                    reranker=False,\n",
    "                    verbose=False,\n",
    "                    test_settings=settings_name,\n",
    "                    rag_prompt_template=RAG_PROMPT_TEMPLATE\n",
    "                )\n",
    "\n",
    "                print(\"Running evaluation...\")\n",
    "                evaluate_answers(\n",
    "                    output_file_name,\n",
    "                    eval_chat_model,\n",
    "                    evaluator_name,\n",
    "                    evaluation_prompt_template,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>test_settings</th>\n",
       "      <th>eval_score_GPT4</th>\n",
       "      <th>eval_feedback_GPT4</th>\n",
       "      <th>settings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what are cameroonian traditionnal meals ?</td>\n",
       "      <td>Cameroon traditional meals : ndole, eru, okok,...</td>\n",
       "      <td>Cameroonian traditional meals include dishes s...</td>\n",
       "      <td>[unique culture in \\nCameroon through its \\nlo...</td>\n",
       "      <td>chunk-100_temperature-0.4_embeddings-all-MiniL...</td>\n",
       "      <td>4</td>\n",
       "      <td>The response correctly identifies several trad...</td>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.4_embeddi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many museum west region of Cameroon has ?</td>\n",
       "      <td>12</td>\n",
       "      <td>The West Region of Cameroon has 15 museums. [...</td>\n",
       "      <td>[The West Region of Cameroon has the highest n...</td>\n",
       "      <td>chunk-100_temperature-0.4_embeddings-all-MiniL...</td>\n",
       "      <td>2</td>\n",
       "      <td>Feedback: The response provided states that th...</td>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.4_embeddi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question  \\\n",
       "0      what are cameroonian traditionnal meals ?   \n",
       "1  How many museum west region of Cameroon has ?   \n",
       "\n",
       "                                         true_answer  \\\n",
       "0  Cameroon traditional meals : ndole, eru, okok,...   \n",
       "1                                                 12   \n",
       "\n",
       "                                    generated_answer  \\\n",
       "0  Cameroonian traditional meals include dishes s...   \n",
       "1   The West Region of Cameroon has 15 museums. [...   \n",
       "\n",
       "                                      retrieved_docs  \\\n",
       "0  [unique culture in \\nCameroon through its \\nlo...   \n",
       "1  [The West Region of Cameroon has the highest n...   \n",
       "\n",
       "                                       test_settings eval_score_GPT4  \\\n",
       "0  chunk-100_temperature-0.4_embeddings-all-MiniL...               4   \n",
       "1  chunk-100_temperature-0.4_embeddings-all-MiniL...               2   \n",
       "\n",
       "                                  eval_feedback_GPT4  \\\n",
       "0  The response correctly identifies several trad...   \n",
       "1  Feedback: The response provided states that th...   \n",
       "\n",
       "                                            settings  \n",
       "0  ./output\\rag_chunk-100_temperature-0.4_embeddi...  \n",
       "1  ./output\\rag_chunk-100_temperature-0.4_embeddi...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result.shape)\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(\n",
    "    lambda x: int(x) if isinstance(x, str) else 1\n",
    ")\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output\\rag_chunk-100_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.194444\n",
       "./output\\rag_chunk-100_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.222222\n",
       "./output\\rag_chunk-100_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.333333\n",
       "./output\\rag_chunk-100_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.361111\n",
       "./output\\rag_chunk-100_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.388889\n",
       "./output\\rag_chunk-100_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.388889\n",
       "./output\\rag_chunk-300_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.500000\n",
       "./output\\rag_chunk-300_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.527778\n",
       "./output\\rag_chunk-300_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.527778\n",
       "./output\\rag_chunk-300_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.527778\n",
       "./output\\rag_chunk-200_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.527778\n",
       "./output\\rag_chunk-200_temperature-0.4_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.527778\n",
       "./output\\rag_chunk-300_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.555556\n",
       "./output\\rag_chunk-200_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.555556\n",
       "./output\\rag_chunk-200_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.555556\n",
       "./output\\rag_chunk-300_temperature-0.8_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.611111\n",
       "./output\\rag_chunk-200_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-False_reader-model-all-MiniLM-L6-v2.json    0.666667\n",
       "./output\\rag_chunk-200_temperature-1.0_embeddings-all-MiniLM-L6-v2_rerank-True_reader-model-all-MiniLM-L6-v2.json     0.694444\n",
       "Name: eval_score_GPT4, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Format and plots results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunk_size(text):\n",
    "    res = text.split(\"rag_chunk-\")[1].split(\"_\")[0]\n",
    "    return res\n",
    "\n",
    "def extract_temperature(text):\n",
    "    res = text.split('temperature-')[1].split(\"_\")[0]\n",
    "    return res\n",
    "\n",
    "def extract_rerank(text):\n",
    "    res = text.split(\"rerank-\")[1].split(\"_\")[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>settings</th>\n",
       "      <th>eval_score_GPT4</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.4_embeddi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.4_embeddi...</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.8_embeddi...</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.8_embeddi...</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-1.0_embeddi...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            settings  eval_score_GPT4  \\\n",
       "0  ./output\\rag_chunk-100_temperature-0.4_embeddi...         0.333333   \n",
       "1  ./output\\rag_chunk-100_temperature-0.4_embeddi...         0.361111   \n",
       "2  ./output\\rag_chunk-100_temperature-0.8_embeddi...         0.388889   \n",
       "3  ./output\\rag_chunk-100_temperature-0.8_embeddi...         0.388889   \n",
       "4  ./output\\rag_chunk-100_temperature-1.0_embeddi...         0.222222   \n",
       "\n",
       "  chunk_size temperature rerank  \n",
       "0        100         0.4  False  \n",
       "1        100         0.4   True  \n",
       "2        100         0.8  False  \n",
       "3        100         0.8   True  \n",
       "4        100         1.0  False  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formated_results = pd.DataFrame(average_scores).reset_index()\n",
    "formated_results['chunk_size'] = formated_results['settings'].apply(extract_chunk_size)\n",
    "formated_results['temperature'] = formated_results['settings'].apply(extract_temperature)\n",
    "formated_results['rerank'] = formated_results['settings'].apply(extract_rerank)\n",
    "formated_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_results[['temperature', 'chunk_size']] = formated_results[['temperature', 'chunk_size']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>settings</th>\n",
       "      <th>eval_score_GPT4</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.4_embeddi...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-0.8_embeddi...</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./output\\rag_chunk-100_temperature-1.0_embeddi...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./output\\rag_chunk-200_temperature-0.4_embeddi...</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./output\\rag_chunk-200_temperature-0.8_embeddi...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            settings  eval_score_GPT4  \\\n",
       "0  ./output\\rag_chunk-100_temperature-0.4_embeddi...         0.333333   \n",
       "2  ./output\\rag_chunk-100_temperature-0.8_embeddi...         0.388889   \n",
       "4  ./output\\rag_chunk-100_temperature-1.0_embeddi...         0.222222   \n",
       "6  ./output\\rag_chunk-200_temperature-0.4_embeddi...         0.527778   \n",
       "8  ./output\\rag_chunk-200_temperature-0.8_embeddi...         0.555556   \n",
       "\n",
       "   chunk_size  temperature rerank  \n",
       "0       100.0          0.4  False  \n",
       "2       100.0          0.8  False  \n",
       "4       100.0          1.0  False  \n",
       "6       200.0          0.4  False  \n",
       "8       200.0          0.8  False  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formated_results = formated_results[formated_results[\"rerank\"]==\"True\"]\n",
    "formated_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "chunk_size=%{x}<br>temperature=%{y}<br>eval_score_GPT4=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           0.3611111111111111,
           0.3888888888888889,
           0.19444444444444445,
           0.5277777777777778,
           0.5555555555555556,
           0.6944444444444444,
           0.5277777777777778,
           0.5555555555555556,
           0.5277777777777778
          ],
          "coloraxis": "coloraxis",
          "size": [
           0.3611111111111111,
           0.3888888888888889,
           0.19444444444444445,
           0.5277777777777778,
           0.5555555555555556,
           0.6944444444444444,
           0.5277777777777778,
           0.5555555555555556,
           0.5277777777777778
          ],
          "sizemode": "area",
          "sizeref": 0.001736111111111111,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          100,
          100,
          100,
          200,
          200,
          200,
          300,
          300,
          300
         ],
         "xaxis": "x",
         "y": [
          0.4,
          0.8,
          1,
          0.4,
          0.8,
          1,
          0.4,
          0.8,
          1
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "eval_score_GPT4"
          }
         },
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ]
        },
        "legend": {
         "itemsizing": "constant",
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "eval score par temperature vs chunk_size"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "chunk_size"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "temperature"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(formated_results, x='chunk_size', y='temperature', \n",
    "                 color='eval_score_GPT4', size='eval_score_GPT4', \n",
    "                 color_continuous_scale='Viridis', \n",
    "                 title='eval score par temperature vs chunk_size')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai-common-app--MoIYZbU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
